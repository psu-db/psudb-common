#! /usr/bin/env bash
# 
# Shuffle the contents of a file externally and in parallel. Should 
# work even if the contents of the file are larger than memory.
#
# Copyright (C) 2023 Douglas Rumbaugh <drumbaugh@psu.edu> 
# 
# All rights reserved. Published under the Revised BSD License.
# 

cleanup() {
    if [[ ! -z ${tmp_dir+x} ]]; then
        rm -r "$tmp_dir"
    fi
}

if [[ $# -lt 2 ]]; then
    echo 'eshuf <input_filename> <output_filename>'
    exit 1
fi

ifname="$1"
ofname="$2"

if [[ $ifname == '-' ]]; then
    echo "ERROR: Script currently does not support reading input from stdin" > /dev/stderr
    exit 1
fi

if [[ $ofname == '-' ]]; then
    echo "ERROR: Script currently does not support writing output to stdout" > /dev/stderr
    exit 1
fi

if [[ ! -f $ifname ]]; then
    printf 'Cannot open input file [%s]\n' "$ifname" > /dev/stderr
    exit 1
fi

if ! touch "$ofname"; then
    printf 'Cannot create output file [%s]\n' "$ofname" >/dev/stderr
    exit 1
fi

rcount=$(wc -l "$ifname")
rcount=${rcount%% *}

# If the count is small, it's better to just generate the data in one pass
if (( rcount <= 1000000 )); then
    < "$ifname" awk 'BEGIN{srand();} {printf "%0.15f\t%s\n", rand(), $0;}' | sort -n | cut  -f 1 --complement - > "$ofname"
    exit 0
fi

# Otherwise, we're going to take a more distributed approach

# We want to use a specific number of subfiles, so we need to know how many
# lines to use per file
fcount=100
lines=$(( rcount / fcount ))

# Create a random dir for temporary files--ensuring it is unique and doesn't
# overlap another running job
tmp_dir="TMP${RANDOM}"
while [[ -d $tmp_prefix ]]; do
    tmp_dir="TMP${RANDOM}"
done

# If the directory create fails at this point, just 
# bail out an retry manually
if ! mkdir "$tmp_dir"; then
    echo "ERROR: Failed to create temporary directory. Ensure you have write access in the working directory." > /dev/stderr
    echo "In rare cases, this could also occur due to a synchronization issue with multiple instances of this script. This situation can be solved by repeating the command." > /dev/stderr
    exit 1
fi

tmp_prefix="$tmp_dir/x"

# register an interrupt handler to clean up the directory if the command
# is cancelled w/ ^C
trap "cleanup" int


# Generate all of the data and decorate each line with a random sorting key,
# then split into fcount number of files for parallel sorting
< "$ifname" awk 'BEGIN{srand();} {printf "%0.15f\t%s\n", rand(), $0;}' | split -l ${lines} - "${tmp_prefix}"

# Sort each file individually based on the random key
parjobs=10
i=0
for f in $tmp_prefix*; do
    (( i++ ))
    { sort -n $f > "${f}.sorted" && rm $f; } &

    if (( i >= parjobs )); then
        wait
        i=0
    fi
done

wait

# We need to ensure that sort can open all of our files at once, otherwise it
# will need to do many intermediate merge steps and may run out of memory.
batch=$(( fcount + 10 ))

# Merge the sorted files together, strip out the sorting key, add a
# line number as a fake second value, and clean out some whitespace
# for easy processing
sort --batch-size="$batch" -n -m "$tmp_dir"/*.sorted | cut -f 1 --complement -  > "$ofname"

# clean up all the extra temporary files
cleanup

